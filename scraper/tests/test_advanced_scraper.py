#!/usr/bin/env python3
"""
Test del scraper avanzado para obtener PDFs reales.
"""
import sys
import os
import pandas as pd

# Agregar directorio scrapers al path
sys.path.append('scrapers')

from advanced_pubmed_scraper import AdvancedPubMedScraper
from github_scraper import GitHubCSVScraper

def main():
    """Funci√≥n principal."""
    print("üöÄ PROBANDO SCRAPER AVANZADO PARA PDFs REALES")
    print("=" * 60)
    
    try:
        # Paso 1: Obtener datos del CSV
        print("\nüìä Paso 1: Obteniendo datos del CSV...")
        github_scraper = GitHubCSVScraper("https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv")
        articles_df = github_scraper.scrape()
        
        print(f"‚úÖ {len(articles_df)} art√≠culos encontrados")
        
        # Paso 2: Crear scraper avanzado
        print("\nüîß Paso 2: Configurando scraper avanzado...")
        scraper = AdvancedPubMedScraper(download_dir="./downloads_real", delay=2.0)
        
        # Paso 3: Procesar primeros 10 art√≠culos
        print("\nüì• Paso 3: Descargando primeros 10 art√≠culos...")
        max_articles = 10
        
        # Mostrar qu√© art√≠culos vamos a procesar
        print(f"\nüìã ART√çCULOS A PROCESAR:")
        for i, (_, row) in enumerate(articles_df.head(max_articles).iterrows()):
            print(f"   {i+1}. {row['Title'][:60]}...")
            print(f"      URL: {row['Link']}")
        
        print(f"\nüöÄ Iniciando descarga avanzada...")
        files = scraper.download_multiple_advanced(articles_df, max_articles=max_articles)
        
        # Paso 4: Resultados
        print(f"\nüìä RESULTADOS:")
        print("=" * 30)
        
        if files:
            print(f"‚úÖ ¬°√âXITO! {len(files)} PDFs reales descargados:")
            
            total_size = 0
            for filepath in files:
                size = os.path.getsize(filepath)
                total_size += size
                filename = os.path.basename(filepath)
                print(f"   üìÑ {filename}")
                print(f"      Tama√±o: {size:,} bytes ({size/1024:.1f} KB)")
                
                # Verificar contenido
                try:
                    with open(filepath, 'rb') as f:
                        header = f.read(10)
                        if header.startswith(b'%PDF'):
                            print(f"      ‚úÖ Header PDF v√°lido")
                        else:
                            print(f"      ‚ùå Header inv√°lido: {header}")
                except Exception as e:
                    print(f"      ‚ùå Error verificando: {e}")
            
            print(f"\nüìà RESUMEN:")
            print(f"   ‚Ä¢ PDFs v√°lidos: {len(files)}/{max_articles}")
            print(f"   ‚Ä¢ Tama√±o total: {total_size:,} bytes ({total_size/1024:.1f} KB)")
            print(f"   ‚Ä¢ Tama√±o promedio: {total_size/len(files):,} bytes ({total_size/len(files)/1024:.1f} KB)")
            
            if total_size/len(files) > 1000000:  # >1MB promedio
                print(f"   üéØ ¬°Los PDFs son REALES! (Promedio >1MB)")
            elif total_size/len(files) > 500000:  # >500KB promedio  
                print(f"   ‚úÖ Los PDFs parecen v√°lidos (Promedio >500KB)")
            else:
                print(f"   ‚ö†Ô∏è PDFs a√∫n muy peque√±os (Promedio <500KB)")
        else:
            print(f"‚ùå No se obtuvieron PDFs v√°lidos")
            print(f"üí° PubMed Central a√∫n est√° bloqueando las descargas")
            print(f"üîß Soluci√≥n definitiva: Instalar Chrome real")
        
    except Exception as e:
        print(f"‚ùå Error en test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
