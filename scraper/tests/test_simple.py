#!/usr/bin/env python3
"""
VersiÃ³n simplificada para pruebas sin dependencias externas complejas.
"""
import sys
import urllib.request
import urllib.parse
import json
import os

def get_csv_data():
    """Obtiene datos del CSV usando solo librerÃ­as estÃ¡ndar."""
    csv_url = "https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv"
    
    print(f"ðŸ” Obteniendo datos de: {csv_url}")
    
    try:
        # Descargar CSV
        response = urllib.request.urlopen(csv_url)
        csv_content = response.read().decode('utf-8')
        
        # Parsear CSV simple
        lines = csv_content.strip().split('\n')
        headers = lines[0].split(',')
        
        print(f"ðŸ“Š Headers encontrados: {headers}")
        print(f"ðŸ“‘ Total de lÃ­neas: {len(lines)}")
        
        # Extraer primeros 5 artÃ­culos
        articles = []
        for i, line in enumerate(lines[1:6], 1):  # Primera lÃ­nea + 5 artÃ­culos
            try:
                # Split simple por comas (puede ser problemÃ¡tico si hay comas en tÃ­tulos)
                parts = line.split(',', 1)  # Dividir mÃ¡ximo 1 vez
                if len(parts) >= 2:
                    articles.append({
                        'title': parts[0].strip('"'),
                        'link': parts[1].strip('"')
                    })
                    print(f"âœ… ArtÃ­culo {i}: {parts[0][:50]}...")
                else:
                    print(f"âš ï¸ LÃ­nea {i} tiene formato incorrecto: {line[:50]}...")
            except Exception as e:
                print(f"âŒ Error procesando lÃ­nea {i}: {e}")
        
        return articles
        
    except Exception as e:
        print(f"âŒ Error descargando CSV: {e}")
        return []

def test_pdf_url(full_article):
    """Prueba si podemos acceder a la URL del artÃ­culo."""
    url = full_article['link']
    title = full_article['title']
    
    print(f"ðŸ”— Probando acceso a: {title[:50]}...")
    print(f"   URL: {url}")
    
    try:
        # Intentar acceder a la pÃ¡gina
        request = urllib.request.Request(url)
        request.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124')
        
        response = urllib.request.urlopen(request, timeout=10)
        content = response.read().decode('utf-8')
        
        print(f"âœ… Acceso exitoso - TamaÃ±o: {len(content)} bytes")
        
        # Buscar links PDF en el contenido
        pdf_links = []
        if '.pdf' in content.lower():
            print("âœ… Se detectÃ³ contenido PDF en la pÃ¡gina")
        else:
            print("âš ï¸ No se detectÃ³ contenido PDF evidente")
            
        # Buscar textos relacionados con PDFs
        pdf_indicators = ['pdf', 'download', 'descarga']
        found_indicators = []
        for indicator in pdf_indicators:
            if indicator.lower() in content.lower():
                found_indicators.append(indicator)
        
        if found_indicators:
            print(f"ðŸ” Indicadores PDF encontrados: {', '.join(found_indicators)}")
        else:
            print("âš ï¸ No se encontraron indicadores claros de descarga PDF")
            
        return True
        
    except Exception as e:
        print(f"âŒ Error accediendo a la URL: {e}")
        return False

def main():
    """FunciÃ³n principal de prueba simplificada."""
    print("ðŸš€ Iniciando prueba simplificada del scraper...")
    print("=" * 60)
    
    # Paso 1: Obtener datos del CSV
    print("ðŸ“Š Paso 1: Obteniendo datos del CSV de GitHub...")
    articles = get_csv_data()
    
    if not articles:
        print("âŒ No se pudieron obtener datos del CSV")
        return 1
    
    print(f"âœ… {len(articles)} artÃ­culos obtenidos exitosamente")
    print()
    
    # Paso 2: Probar acceso a URLs
    print("ðŸ”— Paso 2: Probando acceso a URLs de PubMed Central...")
    successful_access = 0
    
    for i, article in enumerate(articles, 1):
        print(f"ArtÃ­culo {i}/{len(articles)}:")
        if test_pdf_url(article):
            successful_access += 1
        print()
    
    # Resumen
    print("=" * 60)
    print("ðŸ“Š RESUMEN DE PRUEBAS")
    print("=" * 60)
    print(f"âœ… ArtÃ­culos procesados: {len(articles)}")
    print(f"âœ… Accesos exitosos: {successful_access}")
    print(f"ðŸ“ˆ Tasa de Ã©xito: {successful_access/len(articles)*100:.1f}%")
    
    if successful_access > 0:
        print("\nðŸŽ‰ Â¡El scraper bÃ¡sico funciona correctamente!")
        print("ðŸ“‹ Los datos del CSV son accesibles")
        print("ðŸŒ Las URLs de PubMed Central responden correctamente")
        print("\nðŸ’¡ Paso siguiente: Instalar dependencias completas para PDFs")
    else:
        print("\nâš ï¸ Hay problemas con el acceso a las URLs")
        print("ðŸ”§ Verificar conectividad y URLs")
    
    return 0 if successful_access > 0 else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


