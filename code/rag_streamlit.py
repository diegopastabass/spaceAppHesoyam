#!/usr/bin/env python3
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# =========================================================
# 1. Configuraci√≥n inicial
# =========================================================
load_dotenv()
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # evita deadlocks

st.set_page_config(page_title="RAG Chatbot", page_icon="üß¨", layout="wide")
st.title("üß† RAG Chatbot ‚Äî BGE + Azure GPT-4o-mini")
st.caption("Asistente cient√≠fico basado en recuperaci√≥n de informaci√≥n con FAISS + Azure OpenAI")

# Inicializar modelos solo una vez
@st.cache_resource
def load_models():
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    db = FAISS.load_local("vector_db", embeddings, allow_dangerous_deserialization=True)

    llm = AzureChatOpenAI(
        azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        temperature=0.2,
        max_retries=2,
        timeout=None,
    )
    return llm, db

llm, db = load_models()

# =========================================================
# 2. Inicializar sesi√≥n de chat
# =========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar historial
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# =========================================================
# 3. Entrada del usuario
# =========================================================
if question := st.chat_input("Escribe tu pregunta aqu√≠..."):
    # Mostrar pregunta del usuario
    st.chat_message("user").markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    # =========================================================
    # 4. Recuperar documentos relevantes
    # =========================================================
    with st.spinner("üîç Buscando informaci√≥n relevante..."):
        docs_with_scores = db.similarity_search_with_score(question, k=5)

        # Filtrar por umbral de distancia (<= porque menor distancia = m√°s similitud)
        THRESHOLD = 0.35
        relevant_docs = [(doc, score) for doc, score in docs_with_scores if score <= THRESHOLD]

    if not relevant_docs:
        response_text = "‚ö†Ô∏è No se encontraron documentos relevantes. La pregunta est√° fuera de dominio.\n\n**Respuesta:** I don't have enough information to answer."
        with st.chat_message("assistant"):
            st.markdown(response_text)
        st.session_state.messages.append({"role": "assistant", "content": response_text})
    else:
        # Mostrar fuentes relevantes
        st.markdown("### üìö Documentos m√°s relevantes:")
        for i, (d, s) in enumerate(relevant_docs):
            fuente = d.metadata.get("source", "Desconocida")
            st.markdown(f"**[{i+1}]** `{fuente}` ‚Äî distancia: `{s:.4f}`")

        # Crear contexto
        context = "\n\n".join(
            [f"Fuente: {d.metadata.get('source', 'Desconocida')}\n{d.page_content}" for d, _ in relevant_docs]
        )

        # =========================================================
        # 5. Generar respuesta con Azure GPT
        # =========================================================
        prompt = f"""
Eres un asistente cient√≠fico. Responde a la pregunta del usuario **bas√°ndote √∫nicamente** en el siguiente contexto.
Si la informaci√≥n no es suficiente o no es relevante, responde: "No tengo suficiente informaci√≥n para responder."

Por favor, entrega la respuesta en **formato Markdown**, incluyendo:
- Una explicaci√≥n estructurada y clara.
- Uso de subt√≠tulos o vi√±etas si corresponde.
- Destaca nombres de prote√≠nas, genes o v√≠as de se√±alizaci√≥n en *it√°licas* o **negritas**.
- A√±ade un breve resumen al final bajo el t√≠tulo **Resumen**.

---
### üß© Contexto:
{context}

### ‚ùì Pregunta:
{question}

---
### üß† Respuesta en formato Markdown:
"""

        with st.spinner("ü§ñ Generando respuesta..."):
            result = llm.invoke(prompt)
            response_text = result.content

        with st.chat_message("assistant"):
            st.markdown(response_text)

        st.session_state.messages.append({"role": "assistant", "content": response_text})
