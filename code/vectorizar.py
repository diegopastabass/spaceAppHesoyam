import os
import time
from tqdm import tqdm  # ‚úÖ barra de progreso
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings  # ‚úÖ nuevo import
from langchain_community.vectorstores import FAISS

# --------------------------------------------------------
# CONFIGURACI√ìN DE RUTAS
# --------------------------------------------------------

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "extracted_md")
OUTPUT_DIR = os.path.join(BASE_DIR, "vector_db")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --------------------------------------------------------
# TEMPORIZADOR GLOBAL
# --------------------------------------------------------
global_start = time.time()

# --------------------------------------------------------
# CARGA DE DOCUMENTOS
# --------------------------------------------------------

start = time.time()
documents = []
if not os.path.exists(DATA_DIR):
    raise FileNotFoundError(f"‚ùå No se encontr√≥ el directorio {DATA_DIR}")

for file in os.listdir(DATA_DIR):
    if file.endswith(".md"):
        path = os.path.join(DATA_DIR, file)
        with open(path, "r", encoding="utf-8") as f:
            documents.append({"filename": file, "content": f.read()})

print(f"üìÑ {len(documents)} documentos Markdown cargados desde {DATA_DIR}")
print(f"‚è±Ô∏è Tiempo de carga: {time.time() - start:.2f} s\n")

# --------------------------------------------------------
# CHUNKING (divisi√≥n del texto)
# --------------------------------------------------------

start = time.time()
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=150,
    length_function=len
)

chunks = []
print("üß© Dividiendo documentos en chunks...")
for doc in tqdm(documents, desc="Procesando documentos", unit="doc"):
    parts = splitter.split_text(doc["content"])
    for i, part in enumerate(parts):
        chunks.append({
            "filename": doc["filename"],
            "chunk_id": i,
            "content": part
        })

print(f"üß© {len(chunks)} chunks generados en total")
print(f"‚è±Ô∏è Tiempo de chunking: {time.time() - start:.2f} s\n")

# --------------------------------------------------------
# EMBEDDINGS (modelo Hugging Face)
# --------------------------------------------------------

start = time.time()
print("üß† Cargando modelo de embeddings...")
model_name = "BAAI/bge-small-en-v1.5"

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={"device": "cpu"},  # cambia a 'cuda' si tienes GPU
    encode_kwargs={"normalize_embeddings": True}
)
print(f"‚úÖ Modelo cargado en {time.time() - start:.2f} s\n")

# --------------------------------------------------------
# VECTOR STORE (FAISS)
# --------------------------------------------------------

start = time.time()
print("‚öôÔ∏è Generando vectores y creando base FAISS...")

texts = [c["content"] for c in chunks]
metadatas = [{"source": c["filename"], "chunk_id": c["chunk_id"]} for c in chunks]

batch_size = 128
vectors = []
metas = []

for i in tqdm(range(0, len(texts), batch_size), desc="Calculando embeddings", unit="batch"):
    batch_texts = texts[i:i+batch_size]
    batch_metas = metadatas[i:i+batch_size]
    vectors.extend(batch_texts)
    metas.extend(batch_metas)

print("üìä Construyendo √≠ndice FAISS (esto puede tardar varios minutos)...")
build_start = time.time()
vectorstore = FAISS.from_texts(
    texts=vectors,
    embedding=embeddings,
    metadatas=metas
)
print(f"‚úÖ √çndice FAISS construido en {time.time() - build_start:.2f} s")

save_start = time.time()
vectorstore.save_local(OUTPUT_DIR)
print(f"üíæ √çndice guardado en {OUTPUT_DIR} ({time.time() - save_start:.2f} s)\n")
print(f"‚è±Ô∏è Tiempo total de generaci√≥n y guardado FAISS: {time.time() - start:.2f} s\n")

# --------------------------------------------------------
# PRUEBA DE B√öSQUEDA
# --------------------------------------------------------

start = time.time()
query = "How does microgravity affect plant development?"
results = vectorstore.similarity_search(query, k=3)
print(f"‚è±Ô∏è Tiempo de b√∫squeda de prueba: {time.time() - start:.2f} s")

print("\nüîç Resultados de b√∫squeda de prueba:")
for r in results:
    print(f"\nüìò Documento: {r.metadata['source']} (chunk {r.metadata['chunk_id']})")
    print(r.page_content[:300], "...")

# --------------------------------------------------------
# TIEMPO TOTAL GLOBAL
# --------------------------------------------------------

print(f"\nüöÄ Tiempo total de ejecuci√≥n: {time.time() - global_start:.2f} s")
