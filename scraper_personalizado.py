#!/usr/bin/env python3
"""
Scraper personalizable con configuraci√≥n de rango f√°cil.
Modifica START_INDEX y END_INDEX seg√∫n necesites.
"""
import requests
import os
import pandas as pd
import re
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# üîß CONFIGURA TU RANGO AQU√ç - CAMBIA ESTOS VALORES
# ============================================================================
START_INDEX = 1      # N√∫mero del art√≠culo inicial (ej: 1 = primer art√≠culo)
END_INDEX = 10       # N√∫mero del art√≠culo final (ej: 10 = d√©cimo art√≠culo)
# ============================================================================

def validar_configuracion():
    """Valida que la configuraci√≥n sea correcta."""
    if START_INDEX < 1:
        raise ValueError("‚ùå START_INDEX debe ser >= 1")
    if END_INDEX < START_INDEX:
        raise ValueError("‚ùå END_INDEX debe ser >= START_INDEX")
    
    total_articulos = END_INDEX - START_INDEX + 1
    if total_articulos > 50:
        logger.warning(f"‚ö†Ô∏è Rango grande: {total_articulos} art√≠culos")
    
    logger.info(f"üìã Configuraci√≥n v√°lida:")
    logger.info(f"   ‚Ä¢ Art√≠culos: {START_INDEX} a {END_INDEX}")
    logger.info(f"   ‚Ä¢ Total: {total_articulos} art√≠culos")

def obtener_articulos_rango():
    """Obtiene los art√≠culos del rango especificado."""
    csv_url = "https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv"
    df = pd.read_csv(csv_url)
    
    logger.info(f"üìä Dataset completo: {len(df)} art√≠culos")
    
    # Ajustar si END_INDEX excede l√≠mites
    max_end = min(END_INDEX, len(df))
    if END_INDEX > len(df):
        logger.warning(f"‚ö†Ô∏è Ajustando END_INDEX de {END_INDEX} a {max_end}")
    
    # Extraer subconjunto
    subset = df.iloc[START_INDEX-1:max_end].copy()
    
    logger.info(f"üìã Art√≠culos seleccionados: {len(subset)}")
    
    # Mostrar lista
    logger.info(f"\nüìÑ LISTA DE ART√çCULOS:")
    for idx, (_, row) in enumerate(subset.iterrows()):
        global_idx = START_INDEX + idx
        title = str(row.get('Title', ''))[:50]
        url = str(row.get('Link', ''))
        tipo = detectar_tipo_journal(url)
        
        logger.info(f"   #{global_idx:3d}. [{tipo}] {title}...")
    
    return subset

def detectar_tipo_journal(url):
    """Detecta el tipo de journal para mostrar informaci√≥n."""
    if 'pone' in url.lower():
        return "PLoS ONE"
    elif 'ijms' in url.lower():
        return "IJMS"
    elif 'cells' in url.lower():
        return "Cells"
    elif 'nature' in url.lower():
        return "Nature"
    elif 'springer' in url.lower():
        return "Springer"
    else:
        return "Unknown"

def descargar_pdf_real(url, filename):
    """Intenta descargar un PDF real."""
    try:
        # M√©todo 1: Extraer DOI y usar PLoS ONE
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.9'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        content = response.text
        
        # Buscar DOI
        doi_patterns = [
            r'https?://doi\.org/([^\s<>"]+)',
            r'doi:?\s*([^\s<>"]+)',
            r'10\.1371/journal\.pone\.\d+',
        ]
        
        plos_url = None
        for pattern in doi_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if 'pone' in match.lower():
                    pone_match = re.search(r'pone\.(\d+)', match, re.IGNORECASE)
                    if pone_match:
                        pone_id = pone_match.group(1).zfill(7)
                        plos_url = f'https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.{pone_id}&type=printable'
                        break
        
        # M√©todo 2: Construir URL PLoS directamente de PMC ID
        if not plos_url and 'pone' in url.lower():
            pone_match = re.search(r'pone\.(\d+)', url)
            if pone_match:
                pone_id = pone_match.group(1).zfill(7)
                plos_url = f'https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.{pone_id}&type=printable'
        
        if plos_url:
            logger.info(f"üîó Intentando PLoS: {plos_url}")
            
            response = requests.get(plos_url, timeout=60)
            content_type = response.headers.get('content-type', '').lower()
            
            if 'pdf' in content_type and len(response.content) > 50000:
                filepath = os.path.join('downloads', filename)
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                logger.info(f"‚úÖ PDF descargado: {filename} ({len(response.content):,} bytes)")
                return filepath
        
        # Mostrar informaci√≥n detallada cuando falle
        logger.error(f"‚ùå FALLA EN DESCARGA:")
        logger.error(f"   üìÑ Archivo: {filename}")
        logger.error(f"   üîó URL original: {url}")
        if plos_url:
            logger.error(f"   üîó URL PLoS construida: {plos_url}")
        
        # Analizar tipo de journal
        journal_type = detectar_tipo_journal(url)
        logger.error(f"   üì∞ Journal: {journal_type}")
        
        # Sugerir m√©todos alternativos
        logger.error(f"   üí° M√âTODOS ALTERNATIVOS:")
        if 'pone' in url.lower():
            logger.error(f"      ‚Üí Es PLoS ONE pero fall√≥")
            logger.error(f"      ‚Üí Verificar disponibilidad manualmente")
            logger.error(f"      ‚Üí URL directa: {url.replace('/pdf', '/').strip('.pdf')}")
        elif 'ijms' in url.lower():
            logger.error(f"      ‚Üí International Journal of Molecular Sciences")
            logger.error(f"      ‚Üí Requiere m√©todo MDPI espec√≠fico")
            logger.error(f"      ‚Üí URL: https://www.mdpi.com/")
        elif 'cells' in url.lower():
            logger.error(f"      ‚Üí Cells journal")
            logger.error(f"      ‚Üí Requiere m√©todo MDPI espec√≠fico")
            logger.error(f"      ‚Üí URL: https://www.mdpi.com/")
        elif 'nature' in url.lower():
            logger.error(f"      ‚Üí Nature journal")
            logger.error(f"      ‚Üí Requiere m√©todo Nature espec√≠fico")
            logger.error(f"      ‚Üí URL: https://www.nature.com/")
        else:
            logger.error(f"      ‚Üí Journal desconocido: {journal_type}")
            logger.error(f"      ‚Üí Requiere investigaci√≥n manual")
            logger.error(f"      ‚Üí Verificar disponibilidad del PDF")
        
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Error descargando: {e}")
        return None

def procesar_articulos():
    """Procesa todos los art√≠culos del rango."""
    try:
        validar_configuracion()
        
        subset_df = obtener_articulos_rango()
        
        logger.info(f"\nüöÄ INICIANDO DESCARGA...")
        
        exitos = 0
        fallidos = 0
        
        for idx, (_, row) in enumerate(subset_df.iterrows()):
            global_idx = START_INDEX + idx
            title = row['Title']
            url = row['Link']
            
            logger.info(f"\nüìÑ Procesando #{global_idx}: {title[:40]}...")
            
            # Crear nombre archivo
            pmc_match = re.search(r'PMC\d+', url)
            pmc_id = pmc_match.group() if pmc_match else f"PMC{global_idx}"
            safe_title = re.sub(r'[^\w\s-]', '', title.replace(' ', '-')[:30])
            filename = f"{global_idx:03d}-{pmc_id}-{safe_title}.pdf"
            
            # Mostrar informaci√≥n del art√≠culo
            logger.info(f"   üîç Tipo de journal: {detectar_tipo_journal(url)}")
            logger.info(f"   üìù Archivo esperado: {filename}")
            
            # Intentar descarga
            filepath = descargar_pdf_real(url, filename)
            
            if filepath:
                exitos += 1
                logger.info(f"   ‚úÖ RESULTADO: PDF obtenido exitosamente")
            else:
                fallidos += 1
                logger.error(f"   ‚ùå RESULTADO: Fall√≥ descarga - Revisar sugerencias arriba")
        
        # Mostrar resultados
        logger.info(f"\nüéâ PROCESO COMPLETADO!")
        logger.info(f"=" * 40)
        logger.info(f"‚úÖ Exitosos: {exitos}")
        logger.info(f"‚ùå Fallidos: {fallidos}")
        logger.info(f"üìä Tasa: {exitos}/{len(subset_df)} ({exitos/len(subset_df)*100:.1f}%)")
        
        # Mostrar archivos
        if os.path.exists('downloads'):
            files = [f for f in os.listdir('downloads') if f.endswith('.pdf')]
            if files:
                logger.info(f"\nüìÅ ARCHIVOS EN downloads/:")
                total_size = 0
                for file in sorted(files):
                    size = os.path.getsize(os.path.join('downloads', file))
                    total_size += size
                    logger.info(f"   üìÑ {file}: {size:,} bytes ({size/1024/1024:.2f} MB)")
                
                avg_size = total_size / len(files)
                logger.info(f"\nüìè Total: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
                logger.info(f"üìè Promedio: {avg_size:,} bytes ({avg_size/1024/1024:.2f} MB)")
                
                if avg_size > 1_000_000:
                    logger.info(f"üéØ ¬°PDFs REALES obtenidos!")
        
    except Exception as e:
        logger.error(f"‚ùå Error en proceso: {e}")
        import traceback
        traceback.print_exc()

def mostrar_ejemplos():
    """Muestra ejemplos de diferentes configuraciones."""
    logger.info(f"\nüí° EJEMPLOS DE CONFIGURACI√ìN:")
    logger.info(f"   Primeros 5:      START_INDEX = 1,  END_INDEX = 5")
    logger.info(f"   Art√≠culos 10-20: START_INDEX = 10, END_INDEX = 20")
    logger.info(f"   Solo uno:        START_INDEX = 1,  END_INDEX = 1")
    logger.info(f"   Rango medio:     START_INDEX = 50, END_INDEX = 60")
    
    logger.info(f"\nüîß PARA CAMBIAR:")
    logger.info(f"   1. Modifica START_INDEX y END_INDEX en la l√≠nea 12-13")
    logger.info(f"   2. Ejecuta: python3.10 scraper_personalizado.py")

def main():
    """Funci√≥n principal."""
    print("üéØ SCRAPER PERSONALIZADO - RANGO DE ART√çCULOS")
    print("=" * 60)
    
    mostrar_ejemplos()
    
    logger.info(f"\n‚öôÔ∏è CONFIGURACI√ìN ACTUAL:")
    logger.info(f"   START_INDEX = {START_INDEX}")
    logger.info(f"   END_INDEX = {END_INDEX}")
    
    confirmar = input(f"\n¬øProcesar art√≠culos {START_INDEX} a {END_INDEX}? (y/N): ").lower()
    
    if confirmar in ['y', 'yes', 'si', 's']:
        procesar_articulos()
    else:
        logger.info(f"‚ùå Proceso cancelado")
        logger.info(f"üí° Modifica START_INDEX y END_INDEX si quieres cambiar el rango")

if __name__ == "__main__":
    main()
