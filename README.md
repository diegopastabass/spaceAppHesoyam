# ğŸš€ SpaceBio Publications Scraper

Este proyecto descarga automÃ¡ticamente artÃ­culos cientÃ­ficos relacionados con biologÃ­a espacial desde GitHub y PubMed Central, convirtiendo los PDFs a formato Markdown para anÃ¡lisis posterior con IA.

## ğŸ“‹ Funcionalidades

- âœ… **Scraping de GitHub**: Extrae datos del CSV con tÃ­tulos y enlaces de artÃ­culos
- âœ… **Descarga de PDFs**: Descarga automÃ¡tica desde PubMed Central con mÃºltiples mÃ©todos
- âœ… **ConversiÃ³n a Markdown**: Transforma PDFs cientÃ­ficos a formato estructurado
- âœ… **AnÃ¡lisis de contenido**: Ejemplo de anÃ¡lisis de textos cientÃ­ficos
- âœ… **GestiÃ³n de errores**: Sistema robusto con reintentos y manejo de lÃ­mites
- âœ… **Rate limiting**: Respeta lÃ­mites de descarga por hora
- âœ… **Logging completo**: Seguimiento detallado de todas las operaciones

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Scraping Web
- **Requests + BeautifulSoup**: Para pÃ¡ginas estÃ¡ticas
- **Selenium**: Para contenido dinÃ¡mico y navegaciÃ³n automatizada
- **Pandas**: ManipulaciÃ³n de datos CSV

### ConversiÃ³n de Documentos
- **PyPDF2**: ExtracciÃ³n de texto de PDFs
- **Regex**: Procesamiento y limpieza de texto cientÃ­fico
- **Markdown**: Formato estructurado para anÃ¡lisis

### Infraestructura
- **Python 3.8+**: Lenguaje principal
- **WebDriver Manager**: GestiÃ³n automÃ¡tica de drivers
- **Logging**: Sistema completo de registro

## ğŸš« Limitaciones del Web Scraping

### TÃ©cnicas
1. **PolÃ­tica de robots.txt**: Sitios que prohÃ­ben scraping automÃ¡tico
2. **Rate limiting**: LÃ­mites estrictos de peticiones por tiempo
3. **Captchas**: Sistemas anti-bot complejos
4. **Contenido dinÃ¡mico**: JavaScript que requiere ejecuciÃ³n del navegador
5. **Cambios frecuentes**: Sitios que modifican estructura regularmente
6. **Antimalware**: Sistemas que detectan y bloquean bots

### Legales y Ã‰ticas
1. **Copyright**: Respeto a derechos de propiedad intelectual
2. **TÃ©rminos de servicio**: ViolaciÃ³n de condiciones de uso
3. **Privacidad**: ProtecciÃ³n de datos personales
4. **Volumen**: Uso excesivo de recursos del servidor

## ğŸ“Š Flujo del Sistema

```
GitHub CSV â†’ CSV Scraper â†’ ArtÃ­culos List
     â†“
PubMed URLs â†’ PDF Scraper â†’ PDF Downloads
     â†“
PDF Files â†’ Markdown Converter â†’ Structured Text
     â†“
Markdown Files â†’ AI Analysis â†’ Scientific Insights
```

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# Descargar e instalar
git clone <repo-url>
cd spaceAppHesoyam

# Ejecutar instalaciÃ³n automÃ¡tica
./install.sh
```

## ğŸ“– InstalaciÃ³n Manual

```bash
# Instalar dependencias
pip install -r requirements.txt

# Crear directorios
mkdir -p downloads markdown_output logs

# Configurar (editar .env si es necesario)
cp .env.example .env
```

## â–¶ï¸ Uso

### EjecuciÃ³n BÃ¡sica
```bash
# Descargar y convertir artÃ­culos (configurable en .env)
python main.py

# Ejecutar prueba con 3 artÃ­culos
python main.py test
```

### AnÃ¡lisis de Resultados
```bash
# Analizar archivos Markdown generados
python analysis_example.py
```

## ğŸ“ Estructura del Proyecto

```
spaceAppHesoyam/
â”œâ”€â”€ scrapers/              # MÃ³dulos de scraping
â”‚   â”œâ”€â”€ github_scraper.py  # CSV de GitHub
â”‚   â””â”€â”€ pubmed_scraper.py  # PDFs de PubMed Central
â”œâ”€â”€ converters/             # Conversores de formato
â”‚   â””â”€â”€ pdf_to_markdown.py # PDF â†’ Markdown
â”œâ”€â”€ config/                # ConfiguraciÃ³n
â”‚   â””â”€â”€ settings.py        # ConfiguraciÃ³n centralizada
â”œâ”€â”€ downloads/             # PDFs descargados (creado automÃ¡ticamente)
â”œâ”€â”€ markdown_output/       # Archivos Markdown (creado automÃ¡ticamente)
â”œâ”€â”€ logs/                  # Archivos de log (creado automÃ¡tico)
â”œâ”€â”€ main.py               # AplicaciÃ³n principal
â”œâ”€â”€ analysis_example.py   # Ejemplo de anÃ¡lisis con IA
â”œâ”€â”€ requirements.txt      # Dependencias Python
â”œâ”€â”€ install.sh           # Script de instalaciÃ³n
â””â”€â”€ README.md            # Esta documentaciÃ³n
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno (.env)
```env
# URLs y archivos
CSV_FILE_URL=https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publication_PMC.csv

# Directorios
DOWNLOAD_DIR=./downloads
MARKDOWN_OUTPUT_DIR=./markdown_output

# ConfiguraciÃ³n de scraping
MAX_DOWNLOADS_PER_HOUR=50
DELAY_BETWEEN_REQUESTS=2.0
MAX_ARTICLES_TO_DOWNLOAD=10

# User Agent
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64)...

# Logging
LOG_LEVEL=INFO
```

## ğŸ“Š MÃ©todos de Scraping Implementados

### 1. GitHub CSV Scraper
- **Request HTTP** â†’ Obtiene CSV raw desde GitHub
- **Pandas parsing** â†’ Procesa estructura CSV
- **ValidaciÃ³n** â†’ Verifica enlaces PubMed Central
- **Limpieza** â†’ Filtrado de datos inconsistentes

### 2. PubMed PDF Scraper
- **Doble mÃ©todo**: Requests + Selenium
- **DetecciÃ³n automÃ¡tica** de URLs PDF
- **Rate limiting** inteligente (50 descargas/hora)
- **Reintentos** automÃ¡ticos en caso de fallo
- **ValidaciÃ³n** de archivos descargados

### 3. PDF to Markdown Converter
- **ExtracciÃ³n de texto** con PyPDF2
- **IdentificaciÃ³n de secciones** cientÃ­ficas (Abstract, Methods, Results, etc.)
- **Formateo Markdown** estructurado
- **Metadatos** preservados (tÃ­tulo, autor, fecha)

## ğŸ¯ Casos de Uso

1. **InvestigaciÃ³n cientÃ­fica**: Descarga masiva de papers
2. **AnÃ¡lisis bibliomÃ©trico**: Estudios de tendencias en biologÃ­a espacial
3. **Entrada de IA**: PreparaciÃ³n de datos para modelos de lenguaje
4. **Data mining**: ExtracciÃ³n de informaciÃ³n de literatura cientÃ­fica

## ğŸ›¡ï¸ Consideraciones Ã‰ticas

- âš ï¸ **Respetar robots.txt** de otros sitios web
- âš ï¸ **Usar con moderaciÃ³n**: No sobrecargar servidores
- âš ï¸ **Respeta copyright**: Solo para uso acadÃ©mico/investigaciÃ³n
- âš ï¸ **Verifica tÃ©rminos**: Condiciones de uso de cada sitio

## ğŸ”§ ResoluciÃ³n de Problemas

### Error: "No module named ..."
```bash
pip install -r requirements.txt
```

### Error: "WebDriver not found"
```bash
# El sistema instala automÃ¡ticamente ChromeDriver
# Si persiste el error:
pip install --upgrade webdriver-manager
```

### Error: "Permission denied"
```bash
chmod +x install.sh
sudo ./install.sh
```

### Problemas de memoria con muchos PDFs
```bash
# Reducir MAX_ARTICLES_TO_DOWNLOAD en .env
MAX_ARTICLES_TO_DOWNLOAD=5
```

## ğŸ“ˆ Rendimiento Esperado

- **GitHub CSV**: ~100 artÃ­culos en 30 segundos
- **Descarga PDFs**: ~50 PDFs por hora (respetando lÃ­mites)
- **ConversiÃ³n Markdown**: ~1 PDF por segundo
- **AnÃ¡lisis completo**: ~600 artÃ­culos en 14 horas

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Algunas Ã¡reas de mejora:

- [ ] DetecciÃ³n automÃ¡tica de Captchas
- [ ] Soporte para mÃ¡s repositorios acadÃ©micos
- [ ] AnÃ¡lisis de sentimientos cientÃ­fico
- [ ] GeneraciÃ³n automÃ¡tica de resÃºmenes

## ğŸ“„ Licencia

Este proyecto es para uso educativo y de investigaciÃ³n Ãºnicamente. Respeta las polÃ­ticas de uso de los sitios web objetivo.

## ğŸ†˜ Soporte

Si tienes problemas:

1. Revisa los logs en `./logs/scraper.log`
2. Verifica la configuraciÃ³n en `.env`
3. Ejecuta primero `python main.py test`
4. Consulta los errores comunes arriba

---

**Â¡Disfruta explorando la ciencia espacial con IA! ğŸ›¸ğŸ“š**
